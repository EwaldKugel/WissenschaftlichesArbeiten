\subsection{Gefahrenanalyse am Beispiel aktueller Fälle}\label{GefahrenAktuelleFaelle}
% Gefahrenpotentiale an Beispielen konkretisieren: \textbf{Was} wurde gemacht um \textbf{was} zu erreichen?
% Hier Fokus auf Echtzeitproblematik legen? Oder folgt das aus dem nächsten Unterkapitel?
Folgend werden einige aus dem unter Abschnitt \ref{Gefahrenpotential} beschriebenen Gefahren anhand konkreter Realfällen diskutiert. 
Bei diesen Fällen handelt es sich hauptsächlich um Betrugsfälle, die mit Hilfe von Deepfakes durchgeführt wurden.
\par
Einer dieser Fälle, der im Zusammenhang mit Deepfake-Betrug genannt wird, ist die Erbeutung von 220.000\euro{} durch Verwendung eines Audio-Deepfakes.
In diesem Fall stellten Betrüger die Stimme eines CEO am Telefon nach und baten seinen angeblichen Mitarbeiter, den genannten Betrag auf ein Konto zu überweisen \citep[][]{Stupp2019}. 
Der angerufene Mann berichtete nachträglich an die Ermittlungsbeamten, dass er den deutschen Akzent und die Melodie der Stimme seines CEO's erkannte, und somit keinen Verdacht schöpfte dass es sich bei diesem Anrufer um einen Betrüger handelte.
Die Ermittlungen gingen davon aus, dass eine kommerzielle Software zur Erstellung des Deepfakes verwendet wurde.
Dieses Beispiel verdeutlicht, dass jeder mit entsprechender freizugänglicher Software im Stande ist, so einen Betrug mit Hilfe eines Audio-Deepfakes durchzuführen \citep[Vgl.][]{Stupp2019}. 
\par
Eine weitere Betrugsmasche, vor der das FBI offiziell warnte, sind Betrugsfälle in denen sich mit durch Video Deepfake veränderter Stimme für sensible Jobs beworben wird.
Während dieser Jobinterviews, die meist auf Jobs mit Heimarbeit in Softwareunternehmen mit großen Datenmengen abzielen, benutzen Betrüger die Stimme einer anderen Person.
Das FBI betonte hierbei aber, dass die Synchronisation zwischen Lippenbewegung und Sprache nicht komplett übereinstimmte.
Somit gibt es Anhaltspunkte zur Vorbeugung und Erkennung solcher Betrüge in Form von Jobinterviews, indem Mitarbeiter von Unternehmen geschult werden um unter anderem auf solche Merkmale verschärft zu achten \citep[][]{Ferraro2022}. 
\par
Zur Veranschaulichung der potentiellen politschen Manipulation unter der Verwendung von Deepfakes, ist die Rede der amerikanischen Politikerin Nancy Pelosi aus dem Jahr 2019 zu nennen \citep[Vgl.][]{Mervosh2019}.
In dieser Rede wurde das Videomaterial so manipuliert, dass es so scheint als ob sie stotterte und undeutlich rede.
Das manipulierte Video wurde von dem zu dieser Zeit amerikanischen Staatspräsidenten und Anhänger der Gegnerpartei von Nancy Pelosi, Donald Trump, über Twitter verbreitet.
Er teilte es mit den Worten ``Pelosi stammers through news conference'', also mit der gezielten Absicht sie mit Hilfe dieses Videos zu diffamieren \citep[][]{Mervosh2019}.
\par
Dieses Video zeigt die Gefahr der schnellen Verbreitung von ungeprüften Inhalten durch soziale Medien, die unter Abschnitt \ref{Gefahrenpotential} genannt wurde.
Über Facebook wurde dieses Video über 2.5 Millionen mal angeschaut, Facebook selbst lies dieses Video auf der Plattform bestehen versprach aber die Verbreitung einzugrenzen.
Youtube hingegen löschte dieses Video, da es sich um falsche Inhalte handelte.
Dieses unterschiedliche Behandeln von Falschinformation verschiedener sozialer Medien zeigt die Schwierigkeit des Konsums von Informationen über diese Plattformen.
Es bleibt daher ein beliebtes Mittel zur Verbreitung solcher Inhalte, eben aufgrund der freien Verbreitung, Geschwindigkeit und umständlicher Löschung dieser \citep[][]{Appel2022}.
\par
Die hier beschriebenen Betrugsfälle veranschaulichen, dass wie unter Abschnitt \ref{Gefahrenpotential} beschrieben, mit einfachsten Mitteln großer Schaden angerichtet werden kann.
Besonders die von Kietzmann beschriebene Vertrautheit von Inhalten wie z.B. die Stimme im Zusammenspiel mit Gestik und Mimik, ließ bei den erwähnten Beispielen keinen Zweifel an der Echtheit \citep[][]{Kietzmann2020}.
So reichte es bei dem Telefonbetrug, dass der deutsche Akzent und die Melodie der Stimme vermeintlich übereinstimmte, keinen Verdacht zu schöpfen.
Darüberhinaus zeigen die genannten Beispiele, dass die Echtzeit und damit die nahezu nicht vorhandene Zeit zur Erkennung eine entscheidene Rolle spielt.
Einmal verbreitet lässt sich der Schaden nur erschwert beheben oder die Inhalte schwierig Löschen oder Richtigstellen \citep[][]{Shahzad2022}.
Dabei betonte Shahzad einmal mehr, zukünftig effektive Methoden zur Erkennung in Echtzeit zu entwickeln, da diese den Großteil der Bedrohung darstellen.
Ein weiteres Gefahrenpotential bleibt der einfache Zugang zur Erstellung solcher Deepfakes.
Mit den richtigen Zugängen zu Verbreitungskanälen und anzusprechenden Opfern, ist es jedem Menschen möglich das Mittel der Manipulation durch Deepfakes zum Betrug einzusetzen \citep[][]{Appel2022}.


