\section{Einleitung}
% Hier Deepfakes im allgemeinen Vorstellen: Welche Arten gibt es und wie werden die verbreitet?
% Aktuelles Thema als Einführung in unsere konkrete Problematik: Von Deepfake allgemein zu Teilaspekt führen den wir besprechen.
% Enden mit Forschungsfrage bzw. Ziel des Artikel: Was wollen wir zeigen, worauf wollen wir hinaus?
In der heutigen Zeit nimmt das Aufkommen und die Verbreitung von Falschmeldung zu \citep[][]{Hancock2021}.
Im Besonderen mehren sich die mit Hilfe von künstlicher Intelligenz erschaffenen manipulierten Falschmeldungen, sogenannte Deepfakes \citep[][]{Shahzad2022}.
Der Begriff des Deepfake entsteht aus ``Deeplearning'', einer auf künstlicher Intelligenz basierenden Methode des maschinellen Lernens, und ``Fake'' welcher mit Hilfe dieses ``Deeplearnings'' erstellt wird und den Menschen täuschen soll \citep[][]{Mueller2022}.
Innerhalb der Deepfakes werden z.B. Gesichter in eine Bild- oder Videodatei realistisch geschnitten, um diese Personen beliebige Worte sagen zu lassen.
So zeigt ein aktuelles Beispiel, wie Olaf Scholz eine angebliche Rede hält in der er über russische Gaslieferungen spricht \citep[][]{Klasen2022}.
Über Twitter wird die Reaktion Putins auf diese Rede von einer russischen Nachrichtenagentur geteilt \citep[Vgl.][]{Klasen2022}.
Gerade über die Kanäle der sozialen Medien wie Twitter oder Facebook, lassen sich diese Falschmeldungen heutzutage schnell und gezielt verbreiten (siehe Putin-Scholz Beispiel).
\par
Eine weitere Form des Deepfakes ist die Manipulation von Audiodateien (Audio Deepfakes, AD).
Diese Art Deepfake wird oftmals von Betrügern genutzt, um potentielle Opfer am Telefon oder in Interviews zu täuschen, also in Echtzeit \citep[][]{Mueller2022}.
Wie bereits einige Fälle gezeigt haben \citep[vgl.][]{Stupp2019}, nimmt die zeitliche Komponente bei der Erkennung dabei eine besondere Rolle ein.
Denn ist der Deepfake einmal geteilt und in den Köpfen der Menschen, lassen sich Falschmeldungen nur schwierig korrigieren \citep[][]{Hancock2021}.
Dabei ist die nachträgliche Abwendung der Gefahr von Echtzeit-, also oftmals AD, dabei weitaus schwieriger als auf sozialen Medien geteilten Video- oder Bildmaterialien \citep[][]{Shahzad2022}.
\par
% Um Deepfakes als soche zu Erkennung, werden in der Literatur einige Merkmale wie z.B. die Asynchronität von Stimme und Lippenbewegung genannt \citep[][]{Appel2022}.
% Auch das zu häufige Blinzeln, Kopfhaltung oder Falschstellungen von Zähnen, können ein Indiz auf einen Deepfake sein \citep[][]{Shahzad2022}.
Da die menschliche Wahrnehmung zur Erkennung eines Deepfakes allerdings limitiert ist, empfehlen Wissenschaftler den Einsatz von technischen Hilfsmitteln \citep[][]{Mueller2022}.
Zwar wird sich für Maßnahmen ausgesprochen um gegen Deepfakes zu sensibilisieren, aber das wird zukünftig nicht reichen da die Technologie zur Erstellung solcher Deepfakes immer besser wird \citep[][]{Amezaga2022}.
% Daher ist es unbedingt notwendig, dass die Erkennung mit technischen Hilfsmitteln weiterentwickelt werden muss.
% Es gibt einige technische Lösungen zur Erkennung von Audio-Deepfakes, die ähnlich wie die Erstellung von Deepfakes auf künstlicher Intelligenz basieren, welche aber den zeitlichen Faktor nicht berücksichtigen.
\par
In dieser Arbeit wird aufgezeigt, welche Gefahrenpotentiale AD beinhalten, wie die menschliche Wahrnehmung zu solchen Fakes ist und wie die Technik dabei helfen kann diese in Echtzeit zu entlarven.
Dabei werden verschiedene Methoden zur Erstellung und Erkennung solcher Fakes analysiert und diskutiert.
Vor allem werden die beiden Methoden Text-to-Speech und Voice Conversion, welche aktuell überwiegend zur Erzeugung von AD genutzt werden, genauer untersucht.
Die Frage, ob und in welchem Grad AD mit Hilfe aktueller technischer Lösungen in Echtzeit erkennbar sind, wird in mehrere Herausforderungen unterteilt, analysiert und beantwortet.
Dabei ist das Ziel dieser Fragestellung, welche Art von AD mit welchen Ansätzen der technischen Erkennung in welcher Zeit zu erkennen sind und wie die davon abgeleitete zukünftige Behandlung solcher Fakes zu gestalten ist. 
